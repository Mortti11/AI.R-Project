{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageBind and PyTorch are successfully installed and working!\n"
     ]
    }
   ],
   "source": [
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "print(\"ImageBind and PyTorch are successfully installed and working!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8302\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6289, 0.4696, 0.5600],\n",
      "        [0.5459, 0.6150, 0.7046],\n",
      "        [0.3049, 0.7645, 0.9502]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Example tensor\n",
    "tensor = torch.rand(3, 3).to(device)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[\"A car\", \"A man\", \"A cat\"]\n",
    "image_paths=[\"thermal\\SeekThermal\\Test\\car\\003.jpg\", \"thermal\\SeekThermal\\Test\\man\\img_thermal_1576932789724.jpg\", \"thermal\\SeekThermal\\Test\\cat\\877.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageBindModel(\n",
       "  (modality_preprocessors): ModuleDict(\n",
       "    (vision): RGBDTPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 1280), requires_grad=True)\n",
       "      \n",
       "      (rgbt_stem): PatchEmbedGeneric(\n",
       "        (proj): Sequential(\n",
       "          (0): PadIm2Video()\n",
       "          (1): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 257, 1280), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (text): TextPreprocessor(\n",
       "      (pos_embed): tensor((1, 77, 1024), requires_grad=True)\n",
       "      (mask): tensor((77, 77), requires_grad=False)\n",
       "      \n",
       "      (token_embedding): Embedding(49408, 1024)\n",
       "    )\n",
       "    (audio): AudioPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 768), requires_grad=True)\n",
       "      \n",
       "      (rgbt_stem): PatchEmbedGeneric(\n",
       "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10), bias=False)\n",
       "        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 229, 768), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (depth): RGBDTPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 384), requires_grad=True)\n",
       "      \n",
       "      (depth_stem): PatchEmbedGeneric(\n",
       "        (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "        (norm_layer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 197, 384), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (thermal): ThermalPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 768), requires_grad=True)\n",
       "      \n",
       "      (rgbt_stem): PatchEmbedGeneric(\n",
       "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 197, 768), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (imu): IMUPreprocessor(\n",
       "      (pos_embed): tensor((1, 251, 512), requires_grad=True)\n",
       "      (cls_token): tensor((1, 1, 512), requires_grad=True)\n",
       "      \n",
       "      (imu_stem): PatchEmbedGeneric(\n",
       "        (proj): Linear(in_features=48, out_features=512, bias=False)\n",
       "        (norm_layer): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (modality_trunks): ModuleDict(\n",
       "    (vision): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (12): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (13): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (14): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (15): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (16): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (17): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (18): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (19): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (20): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (21): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (22): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (23): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (24): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (25): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (26): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (27): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (28): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (29): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (30): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (31): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (text): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (12): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (13): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (14): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (15): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (16): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (17): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (18): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (19): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (20): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (21): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (22): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (23): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (audio): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.009)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.018)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.027)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.036)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.045)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.055)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.064)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.082)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (depth): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (thermal): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (imu): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.140)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.280)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.420)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.560)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.700)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "  )\n",
       "  (modality_heads): ModuleDict(\n",
       "    (vision): Sequential(\n",
       "      (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=1280, out_features=1024, bias=False)\n",
       "    )\n",
       "    (text): SelectEOSAndProject(\n",
       "      (proj): Sequential(\n",
       "        (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (audio): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=768, out_features=1024, bias=False)\n",
       "    )\n",
       "    (depth): Sequential(\n",
       "      (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=384, out_features=1024, bias=False)\n",
       "    )\n",
       "    (thermal): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=768, out_features=1024, bias=False)\n",
       "    )\n",
       "    (imu): Sequential(\n",
       "      (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (modality_postprocessors): ModuleDict(\n",
       "    (vision): Normalize()\n",
       "    (text): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n",
       "    )\n",
       "    (audio): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=20.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "    (depth): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "    (thermal): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=10.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "    (imu): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
