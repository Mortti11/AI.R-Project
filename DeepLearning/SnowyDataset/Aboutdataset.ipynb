{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; line-height: 1.8; font-size: 18px; padding: 20px; background-color: #F5F5F5; border-radius: 10px; border: 1px solid #ddd;\">\n",
    "    <h1 style=\"color: #2C3E50; text-align: left;\">Dataset: Rain, Snow, and Bad Weather in Traffic Surveillance</h1>\n",
    "    <p style=\"text-align: justify; color: #333;\">Computer vision-based image analysis lays the foundation for automatic traffic surveillance. This works well in daylight when road users are clearly visible to the camera but often struggles when visibility is impaired by insufficient lighting or bad weather conditions such as rain, snow, haze, and fog.</p>\n",
    "    <p style=\"text-align: justify; color: #333;\">In this dataset, it is focused on collecting traffic surveillance video in rainfall and snowfall, capturing <strong>22 five-minute videos</strong> from seven different traffic intersections. The illumination of the scenes varies from broad daylight to twilight and night. The scenes feature glare from headlights of cars, reflections from puddles, and blur from raindrops on the camera lens.</p>\n",
    "    <p style=\"text-align: justify; color: #333;\">It was collected the data using a conventional RGB color camera and a thermal infrared camera. If combined, these modalities should enable robust detection and classification of road users even under challenging weather conditions.</p>\n",
    "    <h2 style=\"color: #2980B9; text-align: left;\">Content and Annotations</h2>\n",
    "    <p style=\"text-align: justify; color: #333;\">The dataset is collected from seven intersections in the Danish cities of Aalborg and Viborg. The RGB and thermal cameras are placed on street lamps, observing the traffic from above. The resolution of both cameras is <strong>640x480 pixels</strong>, and the frame rate is fixed at 20 frames/second. With <strong>21 five-minute sequences</strong> (and one four-minute sequence), this results in <strong>130,800 RGB-thermal image pairs</strong>.</p>\n",
    "    <p style=\"text-align: justify; color: #333;\">The two video streams are synchronized in time, and the images of one modality are transferred to another by a fixed homography. A sample implementation is shown in the file <code>aauRainSnowUtility.py</code>.</p>\n",
    "    <h3 style=\"color: #16A085; text-align: left;\">Annotations</h3>\n",
    "    <p style=\"text-align: justify; color: #333;\">Each instance of a road user is annotated on a pixel-level with a corresponding category label. We have used the AAU VAP Multimodal Pixel Annotator as our annotation tool. The category labels are compatible with the MSCOCO category labels, and the entire annotated dataset is converted to a json-file that can be used directly with the COCO API. In our kernel, we used <code>pycocotools</code> to showcase the dataset and annotations.</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
